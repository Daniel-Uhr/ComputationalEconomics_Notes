{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo Numérico de *Backpropagation*\n",
    "\n",
    "Prof. Daniel de Abreu Pereira Uhr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processo de Aprendizado (Backpropagation):***\n",
    "\n",
    "*Backpropagation* é o algoritmo que calcula como cada peso e *bias* deve ser ajustado, com base no erro da predição em relação ao valor verdadeiro. Seguindo o processo:\n",
    "\n",
    "* Forward Pass:\n",
    "  * Calcula a saída da rede ($\\hat{y}$), dado os inputs.\n",
    "* Cálculo da perda (loss):\n",
    "  * Compara $\\hat{y}$ com o valor verdadeiro ($y$ real), usando uma função de perda.\n",
    "* *Backward Pass* (*Backpropagation*):\n",
    "  * Calcula o gradiente da função de perda em relação a cada peso e bias, usando a regra da cadeia da derivada.\n",
    "* Atualização dos parâmetros:\n",
    "  * Usa esses gradientes para atualizar os pesos, geralmente com algum algoritmo de otimização como Gradiente Descente, Adam (Adaptive Moment Estimation), etc.\n",
    "\n",
    "***Lógica***\n",
    "* Antes da atualização:\n",
    "  * Você tem pesos aleatórios ou iniciais. Eles produzem uma predição ($\\hat{y}$) que, no início, não é boa (ou seja, o erro é grande).\n",
    "* Depois da atualização:\n",
    "  * Os pesos são ajustados um pouquinho na direção que minimiza o erro (seguindo o gradiente da função de perda). Se você repetir isso várias vezes (ao longo de muitas épocas, com muitos exemplos de treino), os pesos convergem para valores que produzem predições melhores.\n",
    "    * Uma época (epoch) é uma passagem completa por todo o conjunto de dados de treinamento.\n",
    "    * Um Lote (Batch) é um subconjunto dos dados de treinamento usado para calcular os gradientes e atualizar os pesos. \n",
    "    * Iteração (iteration) é uma atualização dos pesos, geralmente após processar um lote (batch) de dados.\n",
    "      * iterações totais = número de épocas * número de batches por época.\n",
    "\n",
    "\n",
    "***Exemplo Numérico de Backpropagation:***\n",
    "\n",
    "***Backpropagation manual***\n",
    "\n",
    "Arquitetura:\n",
    "* Inputs: $x_1 = 1$, $x_2 = 2$\n",
    "* Pesos de entrada para o neurônio oculto: $w_1 = 0.1$, $w_2 = 0.2$\n",
    "* Bias da camada oculta: $b_1 = 0.3$\n",
    "* Peso da camada oculta para a saída: $v_1 = 0.4$\n",
    "* Bias da saída: $b_2 = 0.5$\n",
    "* Função de ativação: Identidade (ou seja, $g(z) = z$ e $g'(z) = 1$)\n",
    "* Target (valor verdadeiro): $y = 1$\n",
    "\n",
    "Saída da hidden layer:\n",
    "\n",
    "$$\n",
    "z_1 = (w_1 \\cdot x_1) + (w_2 \\cdot x_2) + b_1 \\\\\n",
    "z_1 = (0.1 \\times 1) + (0.2 \\times 2) + 0.3 = 0.1 + 0.4 + 0.3 = 0.8\n",
    "$$\n",
    "\n",
    "Como a ativação é identidade:\n",
    "\n",
    "$$\n",
    "h_1 = z_1 = 0.8\n",
    "$$\n",
    "\n",
    "**Saída final:**\n",
    "\n",
    "$$\n",
    "\\hat{y} = (v_1 \\cdot h_1) + b_2 = (0.4 \\times 0.8) + 0.5 = 0.32 + 0.5 = 0.82\n",
    "$$\n",
    "\n",
    "Cálculo da perda\n",
    "\n",
    "Usando MSE (Erro Quadrático Médio):\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} (y - \\hat{y})^2 = \\frac{1}{2} (1 - 0.82)^2 = \\frac{1}{2} \\times 0.0324 = 0.0162\n",
    "$$\n",
    "\n",
    "Backward pass (gradientes)\n",
    "\n",
    "**Gradiente da perda em relação a $\\hat{y}$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial \\hat{y}} = -(y - \\hat{y}) = -(1 - 0.82) = -0.18\n",
    "$$\n",
    "\n",
    "**Gradiente em relação a $v_1$ (peso da saída):**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial v_1} = \\frac{\\partial \\text{Loss}}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial v_1} = -0.18 \\times h_1 = -0.18 \\times 0.8 = -0.144\n",
    "$$\n",
    "\n",
    "**Gradiente em relação ao bias da saída $b_2$:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial b_2} = \\frac{\\partial \\text{Loss}}{\\partial \\hat{y}} \\times 1 = -0.18\n",
    "$$\n",
    "\n",
    "**Gradiente em relação a $h_1$ (saída da camada oculta):**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial h_1} = \\frac{\\partial \\text{Loss}}{\\partial \\hat{y}} \\times v_1 = -0.18 \\times 0.4 = -0.072\n",
    "$$\n",
    "\n",
    "**Gradiente em relação a $w_1$ e $w_2$ (pesos da entrada):**\n",
    "\n",
    "Como a ativação da hidden layer também é identidade:\n",
    "\n",
    "Para $w_1$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_1} = \\frac{\\partial \\text{Loss}}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial w_1} = -0.072 \\times x_1 = -0.072 \\times 1 = -0.072\n",
    "$$\n",
    "\n",
    "Para $w_2$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_2} = -0.072 \\times x_2 = -0.072 \\times 2 = -0.144\n",
    "$$\n",
    "\n",
    "Bias da hidden:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial b_1} = -0.072 \\times 1 = -0.072\n",
    "$$\n",
    "\n",
    "Atualização dos pesos (exemplo com learning rate = 0.1)\n",
    "\n",
    "Atualizando $v_1$:\n",
    "\n",
    "$$\n",
    "v_1^{\\text{new}} = v_1 - 0.1 \\times (-0.144) = 0.4 + 0.0144 = 0.4144\n",
    "$$\n",
    "\n",
    "O mesmo processo pode ser aplicado para os outros pesos e bias.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
