{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based Methods (Métodos Baseados em Árvores)\n",
    "\n",
    "Prof. Daniel de Abreu Pereira Uhr\n",
    "\n",
    "### Conteúdo\n",
    "* Introdução\n",
    "* Cross-Validation (Validação Cruzada)\n",
    "  * The Validation Set Approach (A Abordagem do Conjunto de Validação)\n",
    "  * Leave-One-Out Cross-Validation (Validação Cruzada Leave-One-Out)\n",
    "  * k-Fold Cross-Validation (Validação Cruzada k-Fold)\n",
    "* The Bootstrap\n",
    "\n",
    "### Referências\n",
    "\n",
    "* [An Introduction to Statistical Learning](https://www.statlearning.com/) (ISL) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani\n",
    "  * Capítulo 5***\n",
    "* [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/) (ESL) by Trevor Hastie, Robert Tibshirani and Jerome Friedman : \n",
    "  * Capítulo 7\n",
    "\n",
    "***Disclaimer:*** *O material apresentado aqui é uma adaptação do material de aula do Prof. Daniel de Abreu Pereira Uhr, e não deve ser utilizado para fins comerciais. O material é disponibilizado para fins educacionais e de pesquisa, e não deve ser reproduzido sem a devida autorização do autor. Este material pode conter erros e imprecisões. O autor não se responsabiliza por quaisquer danos ou prejuízos decorrentes do uso deste material. O uso deste material é de responsabilidade exclusiva do usuário. Caso você encontre erros ou imprecisões neste material, por favor, entre em contato com o autor para que possam ser corrigidos. O autor agradece qualquer feedback ou sugestão de melhoria.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introdução\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from utils.lecture07 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Trees (Árvores de Decisão)\n",
    "\n",
    "Árvores de decisão envolvem a segmentação do espaço preditor em várias regiões simples . Para fazer uma previsão para uma determinada observação, normalmente usamos a média ou a moda das observações de treinamento na região à qual ela pertence. Como o conjunto de regras de divisão usado para segmentar o espaço preditor pode ser resumido em uma árvore, esses tipos de abordagens são conhecidos como métodos de árvore de decisão.\n",
    "\n",
    "\n",
    "### 2.1. Árvores de Regressão\n",
    "\n",
    "Nesta sessão, consideraremos o Hittersconjunto de dados. Ele consiste em dados individuais de jogadores de beisebol. Em nossas aplicações, estamos interessados ​​em prever os jogadores Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "hitters = pd.read_csv('data/Hitters.csv').dropna()\n",
    "hitters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 linhas × 21 colunas\n",
    "\n",
    "Em particular, estamos interessados ​​em observar como o número de Hitse a Yearsexperiência preveem o Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Features\n",
    "features = ['Years', 'Hits']\n",
    "X = hitters[features].values\n",
    "y = np.log(hitters.Salary.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na verdade, usaremos log(salário), pois ele tem uma distribuição mais gaussiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(11,4))\n",
    "\n",
    "# Plot salary distribution\n",
    "ax1.hist(hitters.Salary.values)\n",
    "ax1.set_xlabel('Salary')\n",
    "ax2.hist(y)\n",
    "ax2.set_xlabel('Log(Salary)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender o que é uma árvore, vamos primeiro dar uma olhada em uma. Ajustamos uma regressão de três com 3 folhas ou, equivalentemente, 2 nós."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression tree\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos plotar os resultados visualmente. A maior vantagem das árvores é a interpretabilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8.1\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_title('Figure 8.1');\n",
    "\n",
    "# Plot tree\n",
    "plot_tree(tree, filled=True, feature_names=features, fontsize=14, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A árvore consiste em uma série de regras de divisão, começando no topo da árvore. A divisão superior atribui observações com Years<4,5 ao ramo esquerdo.¹ O salário previsto para esses jogadores é dado pelo valor médio da resposta para os jogadores no conjunto de dados com Years<4,5. Para esses jogadores, o logaritmo médio do salário é 5,107, e, portanto, fazemos uma previsão de 5,107 milhares de dólares, ou seja, US$ 165.174, para esses jogadores. Jogadores com Years>=4,5 são atribuídos ao ramo direito, e então esse grupo é subdividido por Hits.\n",
    "\n",
    "No geral, a árvore estratifica ou segmenta os jogadores em três regiões do espaço preditor:\n",
    "\n",
    "jogadores que jogaram por quatro anos ou menos\n",
    "jogadores que jogaram por cinco ou mais anos e que fizeram menos de 118 rebatidas no ano passado, e\n",
    "jogadores que jogaram por cinco anos ou mais e que fizeram pelo menos 118 rebatidas no ano passado.\n",
    "Essas três regiões podem ser escritas como\n",
    "\n",
    "R1 = {X | Years<4,5}\n",
    "R2 = {X | Years>=4,5, Hits<117,5}, e\n",
    "R3 = {X | Years>=4,5, Hits>=117,5}.\n",
    "Desde a dimensão de\n",
    "é 2, podemos visualizar o espaço e as regiões em um gráfico bidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8.2\n",
    "def make_figure_8_2():\n",
    "    \n",
    "    # Init\n",
    "    hitters.plot('Years', 'Hits', kind='scatter', color='orange', figsize=(7,6))\n",
    "    plt.title('Figure 8.2')\n",
    "    plt.xlim(0,25); plt.ylim(ymin=-5);\n",
    "    plt.xticks([1, 4.5, 24]); plt.yticks([1, 117.5, 238]);\n",
    "\n",
    "    # Split lines\n",
    "    plt.vlines(4.5, ymin=-5, ymax=250, color='g')\n",
    "    plt.hlines(117.5, xmin=4.5, xmax=25, color='g')\n",
    "\n",
    "    # Regions\n",
    "    plt.annotate('R1', xy=(2,117.5), fontsize='xx-large')\n",
    "    plt.annotate('R2', xy=(11,60), fontsize='xx-large')\n",
    "    plt.annotate('R3', xy=(11,170), fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figure_8_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos interpretar a árvore de regressão acima da seguinte forma: os anos são o fator mais importante na determinação do salário, e jogadores com menos experiência ganham salários mais baixos do que jogadores mais experientes. Dado que um jogador é menos experiente, o número de rebatidas que ele acertou no ano anterior parece ter pouca influência em seu salário. Mas entre jogadores que estão nas ligas principais há cinco anos ou mais, o número de rebatidas no ano anterior afeta o salário, e jogadores que acertaram mais rebatidas no ano anterior tendem a ter salários mais altos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Construindo uma árvore***\n",
    "\n",
    "Existem duas etapas principais na construção de uma árvore:\n",
    "\n",
    "Dividimos o espaço preditor, ou seja, o conjunto de valores possíveis para\n",
    "em\n",
    "regiões distintas e não sobrepostas,\n",
    ".\n",
    "Para cada observação que cai na região\n",
    ", fazemos a mesma previsão, que é simplesmente a média dos valores de resposta para as observações de treinamento em\n",
    ".\n",
    "O segundo passo é fácil. Mas como construir as regiões? Nosso objetivo é minimizar a Soma dos Resíduos Quadrados, nas diferentes regiões:\n",
    "\n",
    "$$ \\sum_{j=1}^{J} \\sum_{i \\em R_{j}}\\esquerda(y_{i}-\\que{y} {R {j}}\\direita)^{2} $$\n",
    "\n",
    "Infelizmente, é computacionalmente inviável considerar todas as partições possíveis do espaço de recursos em\n",
    "caixas.\n",
    "\n",
    "Por esse motivo, adotamos uma abordagem gananciosa de cima para baixo , conhecida como divisão binária recursiva . A abordagem é de cima para baixo porque começa no topo da árvore (ponto em que todas as observações pertencem a uma única região) e, em seguida, divide sucessivamente o espaço preditor; cada divisão é indicada por meio de dois novos ramos mais abaixo na árvore. Ela é gananciosa porque, a cada etapa do processo de construção da árvore, a melhor divisão é feita naquela etapa específica, em vez de olhar para o futuro e escolher uma divisão que levará a uma árvore melhor em alguma etapa futura.\n",
    "\n",
    "Na prática, o método é o seguinte:\n",
    "\n",
    "nós selecionamos o preditor\n",
    "nós selecionamos o ponto de corte\n",
    "de modo que a divisão do espaço preditor nas regiões\n",
    "e\n",
    "leva à maior redução possível em RSS\n",
    "repetimos (1)-(2) para todos os preditores\n",
    ", ou seja, resolvemos\n",
    "$$ \\arg \\min_{j,s} \\ \\sum_{i: x_{i} \\em {X|X_j < s}}\\esquerda(y_{i}-\\que{y} i\\direita)^{2}+\\soma {i: x_{i} \\em {X|X_j \\geq s}}\\esquerda(y_{i}-\\que{y}_i\\direita)^{2} $$\n",
    "\n",
    "escolhemos o preditor e o ponto de corte de forma que a árvore resultante tenha o menor RSS\n",
    "Continuamos repetindo (1)-(4) até que uma determinada condição seja atendida. No entanto, após a primeira iteração, também precisamos escolher qual região dividir, o que adiciona uma dimensão adicional para otimizar.\n",
    "Vamos criar nossa própria Nodeclasse para brincar com árvores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Class used to represent nodes in a Regression Tree\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    x : np.array\n",
    "        independent variables\n",
    "    y : np.array\n",
    "        dependent variables\n",
    "    idxs : np.array\n",
    "        indexes fo x and y for current node\n",
    "    depth : int\n",
    "        depth of the sub-tree (default 5)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    find_next_nodes(self)\n",
    "        Keep growing the tree\n",
    "        \n",
    "    find_best_split(self)\n",
    "        Find the best split\n",
    "        \n",
    "    split(self)\n",
    "        Split the tree\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, y, idxs, depth=5):\n",
    "        \"\"\"Initialize node\"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.idxs = idxs \n",
    "        self.depth = depth\n",
    "        self.get_next_nodes()\n",
    "\n",
    "    def get_next_nodes(self):\n",
    "        \"\"\"If the node is not terminal, get further splits\"\"\"\n",
    "        if self.is_last_leaf: return \n",
    "        self.find_best_split()       \n",
    "        self.split()             \n",
    "        \n",
    "    def find_best_split(self):\n",
    "        \"\"\"Loop over variables and their values to find the best split\"\"\"\n",
    "        best_score = float('inf')\n",
    "        # Loop over variables\n",
    "        for col in range(self.x.shape[1]):\n",
    "            x = self.x[self.idxs, col]\n",
    "            # Loop over all splits\n",
    "            for s in np.unique(x):\n",
    "                lhs = x <= s\n",
    "                rhs = x > s\n",
    "                curr_score = self.get_score(lhs, rhs)\n",
    "                # If best score, save it \n",
    "                if curr_score < best_score: \n",
    "                    best_score = curr_score\n",
    "                    self.split_col = col\n",
    "                    self.split_val = s\n",
    "        return self\n",
    "    \n",
    "    def get_score(self, lhs, rhs):\n",
    "        \"\"\"Get score of a given split\"\"\"\n",
    "        y = self.y[self.idxs]\n",
    "        lhs_mse = self.get_mse(y[lhs])\n",
    "        rhs_mse = self.get_mse(y[rhs])\n",
    "        return lhs_mse * lhs.sum() + rhs_mse * rhs.sum()\n",
    "        \n",
    "    def get_mse(self, y): return np.mean((y-np.mean(y))**2)\n",
    "    \n",
    "    def split(self): \n",
    "        \"\"\"Split a node into 2 sub-nodes (recursive)\"\"\"\n",
    "        x = self.x[self.idxs, self.split_col]\n",
    "        lhs = x <= self.split_val\n",
    "        rhs = x > self.split_val\n",
    "        self.lhs = Node(self.x, self.y, self.idxs[lhs], self.depth-1)\n",
    "        self.rhs = Node(self.x, self.y, self.idxs[rhs], self.depth-1)\n",
    "        to_print = (self.depth, self.split_col, self.split_val, sum(lhs), sum(rhs))\n",
    "        print('Split on layer %.0f: var%1.0f = %.4f (%.0f/%.0f)' % to_print)\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def is_last_leaf(self): return self.depth<=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual é a Nodeaparência de um?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init first node\n",
    "tree1 = Node(X, y, np.arange(len(y)), 1)\n",
    "\n",
    "# Documentation (always comment and document your code!)\n",
    "print(tree1.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quais propriedades ele possui?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the class\n",
    "dir(tree1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual é a profundidade? Quantas observações existem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info\n",
    "print('Tree of depth %.0f with %.0f observations' % (tree1.depth, len(tree1.idxs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tudo bem, a árvore é apenas uma folha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if terminal\n",
    "tree1.is_last_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos encontrar a primeira divisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best split\n",
    "tree1.find_best_split()\n",
    "print('Split at var%1.0f = %.4f' % (tree1.split_col, tree1.split_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e tiver selecionado a primeira variável, no valor\n",
    ".\n",
    "\n",
    "Se chamarmos a splitfunção, ela também nos dirá quantas observações por folha a divisão gera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tree\n",
    "tree1.split();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora estamos prontos para calcular árvores ainda mais profundas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check depth-3 tree\n",
    "tree3 = Node(X, y, np.arange(len(y)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poda\n",
    "\n",
    "O processo descrito acima pode produzir boas previsões no conjunto de treinamento, mas provavelmente sobreajustará os dados, levando a um desempenho ruim do conjunto de teste. Isso ocorre porque a árvore resultante pode ser muito complexa. Uma árvore menor, com menos divisões, pode levar a uma menor variância e melhor interpretação, mas com um pequeno viés.\n",
    "\n",
    "Podemos ver isso acontecendo se construirmos a mesma árvore acima, mas com 5 folhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tree\n",
    "overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós plotamos a árvore de 5 folhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree\n",
    "fig, ax = plt.subplots(1,1)\n",
    "plot_tree(overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A divisão na extrema esquerda prevê um valor muito alto Salary(7,243) para jogadores com pouca Yearsexperiência e poucos Hits. De fato, essa previsão se baseia em uma subamostra extremamente pequena (2). Eles provavelmente são outliers e nossa árvore provavelmente está superajustada.\n",
    "\n",
    "Uma alternativa possível é inserir um número mínimo de observações por folha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tree\n",
    "no_overfit_tree = DecisionTreeRegressor(max_leaf_nodes=5, min_samples_leaf=10).fit(X, y)\n",
    "\n",
    "# Plot tree\n",
    "fig, ax = plt.subplots(1,1)\n",
    "plot_tree(no_overfit_tree, filled=True, feature_names=features, fontsize=14, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora a árvore faz muito mais sentido: quanto menor o Yearse o Hits, menor o previsto Salary, como podemos ver pelos tons ficando cada vez mais escuros à medida que nos movemos da esquerda para a direita\n",
    "\n",
    "Outra alternativa possível ao processo descrito acima é construir a árvore somente enquanto a diminuição no RSS devido a cada divisão exceder algum limite (alto).\n",
    "\n",
    "Essa estratégia resultará em árvores menores, mas é muito míope, pois uma divisão aparentemente inútil no início da árvore pode ser seguida por uma divisão muito boa, ou seja, uma divisão que leva a uma grande redução no RSS mais tarde.\n",
    "\n",
    "Podemos usar a validação cruzada para escolher o comprimento ideal da árvore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import original split\n",
    "features = ['Years', 'Hits', 'RBI', 'PutOuts', 'Walks', 'Runs', 'AtBat', 'HmRun']\n",
    "X_train = pd.read_csv('data/Hitters_X_train.csv').dropna()[features]\n",
    "X_test = pd.read_csv('data/Hitters_X_test.csv').dropna()[features]\n",
    "y_train = pd.read_csv('data/Hitters_y_train.csv').dropna()\n",
    "y_test = pd.read_csv('data/Hitters_y_test.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "params = range(2,11)\n",
    "reg_scores = np.zeros((len(params),3))\n",
    "best_score = 10**6\n",
    "\n",
    "# Loop over all parameters\n",
    "for i,k in enumerate(params):\n",
    "    \n",
    "    # Model\n",
    "    tree = DecisionTreeRegressor(max_leaf_nodes=k)\n",
    "\n",
    "    # Loop over splits\n",
    "    tree.fit(X_train, y_train)\n",
    "    reg_scores[i,0] = mean_squared_error(tree.predict(X_train), y_train)\n",
    "    reg_scores[i,1] = mean_squared_error(tree.predict(X_test), y_test)\n",
    "\n",
    "    # Get CV score\n",
    "    kf6 = KFold(n_splits=6)\n",
    "    reg_scores[i,2] = -cross_val_score(tree, X_train, y_train, cv=kf6, scoring='neg_mean_squared_error').mean()\n",
    "    \n",
    "    # Save best model\n",
    "    if reg_scores[i,2]<best_score:\n",
    "        best_model = tree\n",
    "        best_score = reg_scores[i,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos traçar a profundidade ideal da árvore usando cv de 6 vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8.5\n",
    "def make_figure_8_5():\n",
    "    \n",
    "    # Init\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,6))\n",
    "    fig.suptitle('Figure 8.5')\n",
    "\n",
    "    # Plot scores\n",
    "    ax1.plot(params, reg_scores);\n",
    "    ax1.axvline(params[np.argmin(reg_scores[:,2])], c='k', ls='--')\n",
    "    ax1.legend(['Train','Test','6-fold CV']);\n",
    "    ax1.set_title('Cross-Validation Scores');\n",
    "\n",
    "    # Plot best tree\n",
    "    plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);\n",
    "    ax2.set_title('Best Model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figure_8_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árvores de Classificação\n",
    "\n",
    "\n",
    "Uma árvore de classificação é muito semelhante a uma árvore de regressão, exceto que ela é usada para prever uma resposta qualitativa em vez de quantitativa.\n",
    "\n",
    "Para uma árvore de classificação, prevemos que cada observação pertence à classe de observações de treinamento mais comum na região à qual ela pertence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construindo uma Árvore de Classificação\n",
    "A tarefa de desenvolver uma árvore de classificação é semelhante à de desenvolver uma árvore de regressão. No entanto, no contexto de classificação, o RSS não pode ser usado como critério para realizar as divisões binárias.\n",
    "\n",
    "Nós definimos\n",
    "como a proporção de observações de treinamento no\n",
    "região que são da\n",
    "classe. Possíveis funções de perda para decidir as divisões são:\n",
    "\n",
    "Taxa de erro de classificação\n",
    "$$ E = 1 - \\max {k}\\esquerda(\\hat{p} {mk}\\direita) $$\n",
    "\n",
    "Índice de Gini\n",
    "$$ G=\\sum_{k=1}^{K} \\hat{p} {mk}\\esquerda(1-\\hat{p} {mk}\\direita) $$\n",
    "\n",
    "Entropia\n",
    "$$ D=-\\sum_{k=1}^{K} \\hat{p} {mk} \\log \\hat{p} {mk} $$\n",
    "\n",
    "Em problemas de classificação de 2 classes, é assim que as diferentes pontuações se parecem, para diferentes proporções da classe 2 (\n",
    "), quando a verdadeira proporção é\n",
    ".\n",
    "\n",
    "Desenho\n",
    "Ao construir uma árvore de classificação, o índice de Gini ou a entropia são normalmente usados ​​para avaliar a qualidade de uma divisão específica, já que essas duas abordagens são mais sensíveis à pureza do nó do que a taxa de erro de classificação.\n",
    "\n",
    "Nesta seção, trabalharemos com o Heartconjunto de dados sobre insuficiências cardíacas individuais. Tentaremos usar características individuais para prever doenças cardíacas ( HD). A variável é binária: Sim, Não."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load heart dataset\n",
    "heart = pd.read_csv('data/Heart.csv').drop('Unnamed: 0', axis=1).dropna()\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastorize variables\n",
    "heart.ChestPain = pd.factorize(heart.ChestPain)[0]\n",
    "heart.Thal = pd.factorize(heart.Thal)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features\n",
    "features = [col for col in heart.columns if col!='AHD']\n",
    "X2 = heart[features]\n",
    "y2 = pd.factorize(heart.AHD)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora ajustamos nosso classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit classification tree\n",
    "clf = DecisionTreeClassifier(max_depth=None, max_leaf_nodes=11)\n",
    "clf.fit(X2,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual é a pontuação?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final score\n",
    "clf.score(X2,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada na árvore inteira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8.6 a\n",
    "def make_fig_8_6a():\n",
    "    \n",
    "    # Init\n",
    "    fig, ax = plt.subplots(1,1, figsize=(16,12))\n",
    "    ax.set_title('Figure 8.6');\n",
    "\n",
    "    # Plot tree\n",
    "    plot_tree(clf, filled=True, feature_names=features, class_names=['No','Yes'], fontsize=12, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_8_6a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta figura tem uma característica surpreendente: algumas das divisões produzem dois nós terminais que têm o mesmo valor previsto.\n",
    "\n",
    "Por exemplo, considere a divisão Age<=57,5 próxima ao canto inferior esquerdo da árvore não podada. Independentemente do valor de Age, um valor de resposta de Não é previsto para essas observações. Por que, então, a divisão é realizada?\n",
    "\n",
    "A divisão é realizada porque leva ao aumento da pureza do nó. Ou seja, 2/81 das observações correspondentes à folha da esquerda têm um valor de resposta de Sim , enquanto 9/36 daquelas correspondentes à folha da direita têm um valor de resposta de Sim . Por que a pureza do nó é importante? Suponha que temos uma observação de teste que pertence à região dada por essa folha da esquerda. Então podemos ter quase certeza de que seu valor de resposta é Não . Em contraste, se uma observação de teste pertence à região dada pela folha da direita, então seu valor de resposta é provavelmente Não , mas temos muito menos certeza. Mesmo que a divisão Age<=57,5 não reduza o erro de classificação, ela melhora o índice de Gini e a entropia, que são mais sensíveis à pureza do nó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poda para Classificação\n",
    "\n",
    "Podemos repetir o exercício de poda também para a tarefa de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8.6 b\n",
    "def make_figure_8_6b():\n",
    "    \n",
    "    # Init\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,6))\n",
    "    fig.suptitle('Figure 8.6')\n",
    "\n",
    "    # Plot scores\n",
    "    ax1.plot(params, clf_scores);\n",
    "    ax1.legend(['Train','Test','6-fold CV']);\n",
    "\n",
    "    # Plot best tree\n",
    "    plot_tree(best_model, filled=True, impurity=False, feature_names=features, fontsize=12, ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "J = 10\n",
    "params = range(2,11)\n",
    "clf_scores = np.zeros((len(params),3))\n",
    "best_score = 100\n",
    "\n",
    "# Loop over all parameters\n",
    "for i,k in enumerate(params):\n",
    "    \n",
    "    # Model\n",
    "    tree = DecisionTreeClassifier(max_leaf_nodes=k)\n",
    "    \n",
    "    # Loop J times\n",
    "    temp_scores = np.zeros((J,3))\n",
    "    for j in range (J):\n",
    "\n",
    "        # Loop over splits\n",
    "        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)\n",
    "        m = tree.fit(X2_train, y2_train)\n",
    "        temp_scores[j,0] = mean_squared_error(m.predict(X2_train), y2_train)\n",
    "        temp_scores[j,1] = mean_squared_error(m.predict(X2_test), y2_test)\n",
    "\n",
    "        # Get CV score\n",
    "        kf6 = KFold(n_splits=6)\n",
    "        temp_scores[j,2] = -cross_val_score(tree, X2_train, y2_train, cv=kf6, scoring='neg_mean_squared_error').mean()\n",
    "        \n",
    "        # Save best model\n",
    "        if temp_scores[j,2]<best_score:\n",
    "            best_model = m\n",
    "            best_score = temp_scores[j,2]\n",
    "        \n",
    "    # Average\n",
    "    clf_scores[i,:] = np.mean(temp_scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figure_8_6b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outras questões\n",
    "Valores preditivos ausentes\n",
    "Geralmente há duas maneiras principais de lidar com valores ausentes:\n",
    "\n",
    "descartar as observações\n",
    "preencha os valores ausentes com previsões usando as outras observações (por exemplo, média)\n",
    "Com árvores podemos fazer melhor:\n",
    "\n",
    "codificá-los como uma classe separada (por exemplo, 'ausente')\n",
    "gerar divisões usando dados não ausentes e usar variáveis ​​não ausentes em dados ausentes para imitar as divisões com dados ausentes\n",
    "Preditores Categóricos\n",
    "Ao dividir um preditor com q valores possíveis não ordenados, há\n",
    "possíveis partições dos valores q em dois grupos, e os cálculos tornam-se proibitivos para grandes\n",
    ". No entanto, com um\n",
    "resultado, esse cálculo simplifica.\n",
    "\n",
    "Divisões de Combinação Linear\n",
    "Em vez de restringir as divisões para serem do tipo\n",
    ", pode-se permitir divisões ao longo de combinações lineares da forma\n",
    ". Os pesos\n",
    "tornar-se parte do procedimento de otimização.\n",
    "\n",
    "Outros procedimentos de construção de árvores\n",
    "O procedimento que vimos para construir árvores é chamado CART (Árvore de Classificação e Regressão). Existem outros procedimentos.\n",
    "\n",
    "A Matriz de Perdas\n",
    "Em relação a outros métodos, a escolha das funções de perda desempenha um papel muito mais importante.\n",
    "\n",
    "Divisões binárias\n",
    "Você pode fazer divisões não binárias, mas no final elas são apenas versões mais fracas das divisões binárias.\n",
    "\n",
    "Instabilidade\n",
    "As árvores têm uma variância muito alta .\n",
    "\n",
    "Dificuldade em capturar a estrutura aditiva\n",
    "Árvores são muito ruins em modelar estruturas aditivas.\n",
    "\n",
    "Falta de suavidade\n",
    "As árvores não são lisas.\n",
    "\n",
    "Árvores vs Regressão\n",
    "Vantagens\n",
    "\n",
    "Árvores são muito fáceis de explicar às pessoas. Aliás, são ainda mais fáceis de explicar do que a regressão linear!\n",
    "Algumas pessoas acreditam que as árvores de decisão refletem mais de perto a tomada de decisão humana do que as abordagens de regressão e classificação vistas nos capítulos anteriores.\n",
    "As árvores podem ser exibidas graficamente e são facilmente interpretadas até mesmo por um não especialista (especialmente se forem pequenas).\n",
    "As árvores podem manipular facilmente preditores qualitativos sem a necessidade de criar variáveis ​​fictícias.\n",
    "Desvantagens\n",
    "\n",
    "árvores geralmente não têm o mesmo nível de precisão preditiva que algumas das outras abordagens de regressão e classificação vistas neste livro.\n",
    "Árvores podem ser muito pouco robustas. Em outras palavras, uma pequena alteração nos dados pode causar uma grande alteração na árvore estimada final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Bagging, Florestas Aleatórias, Boosting\n",
    "\n",
    "\n",
    "Bagging, florestas aleatórias e boosting usam árvores como blocos de construção para construir modelos de previsão mais poderosos.\n",
    "\n",
    "Ensacamento\n",
    "O principal problema das árvores de decisão é que elas sofrem com alta variância . A agregação bootstrap , ou bagging , é um procedimento de uso geral para reduzir a variância de um método de aprendizado estatístico.\n",
    "\n",
    "A ideia principal por trás do bagging é que, dado um conjunto de n observações independentes\n",
    ", cada um com variância\n",
    ", a variância da média\n",
    "das observações é dado por\n",
    ". Em outras palavras, calcular a média de um conjunto de observações reduz a variância.\n",
    "\n",
    "De fato, o bagging consiste em pegar muitos conjuntos de treinamento da população, construir um modelo de previsão separado usando cada conjunto de treinamento e calcular a média das previsões resultantes . Como não temos acesso a muitos conjuntos de treinamento, recorremos ao bootstrapping.\n",
    "\n",
    "Estimativa de erro fora da sacola\n",
    "Acontece que existe uma maneira muito simples de estimar o erro de teste de um modelo ensacado, sem a necessidade de realizar validação cruzada ou a abordagem do conjunto de validação. Lembre-se de que a chave para o ensaque é que as árvores são repetidamente ajustadas a subconjuntos bootstrapped das observações. Pode-se mostrar que, em média, cada árvore ensacada utiliza cerca de dois terços das observações. O terço restante das observações não utilizadas para ajustar uma determinada árvore ensacada são chamadas de observações fora do ensaque (OOB). Podemos prever a resposta para a i-ésima observação usando cada uma das árvores nas quais essa observação foi OOB.\n",
    "\n",
    "Agora vamos calcular o índice de Gini para o Heartconjunto de dados usando diferentes números de árvores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init (takes a lot of time with J=30)\n",
    "params = range(2,50)\n",
    "bagging_scores = np.zeros((len(params),2))\n",
    "J = 30;\n",
    "\n",
    "# Loop over parameters\n",
    "for i, k in enumerate(params):\n",
    "    print(\"Computing k=%1.0f\" % k, end =\"\")\n",
    "    \n",
    "    # Repeat J \n",
    "    temp_scores = np.zeros((J,2))\n",
    "    for j in range(J):\n",
    "        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)\n",
    "        bagging = BaggingClassifier(DecisionTreeClassifier(), max_samples=k, oob_score=True)\n",
    "        bagging.fit(X2_train,y2_train)\n",
    "        temp_scores[j,0] = bagging.score(X2_test, y2_test)\n",
    "        temp_scores[j,1] = bagging.oob_score_\n",
    "        \n",
    "    # Average\n",
    "    bagging_scores[i,:] = np.mean(temp_scores, axis=0)\n",
    "    print(\"\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos traçar o erro Out-of-Bag calculado durante a geração do estimador ensacado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new figure 1\n",
    "def make_new_figure_1():\n",
    "    \n",
    "    # Init\n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,6))\n",
    "    fig.suptitle(\"Estimated $R^2$\")\n",
    "\n",
    "    # Plot scores\n",
    "    ax.plot(params, bagging_scores);\n",
    "    ax.legend(['Test','OOB']);\n",
    "    ax.set_xlabel('Number of Trees'); ax.set_ylabel('R^2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_figure_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se demonstrar que, com B suficientemente grande, o erro OOB é virtualmente equivalente ao erro de validação cruzada \"leave-one-out\". A abordagem OOB para estimar o erro de teste é particularmente conveniente ao realizar bagging em grandes conjuntos de dados para os quais a validação cruzada seria computacionalmente onerosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medidas de Importância Variável\n",
    "\n",
    "Como discutimos, a principal vantagem do bagging é reduzir a variância da previsão. No entanto, com o bagging, pode ser difícil interpretar o modelo resultante. Na verdade, não podemos mais desenhar árvores, pois temos muitas delas.\n",
    "\n",
    "No entanto, é possível obter um resumo geral da importância de cada preditor usando o RSS (para árvores de regressão bagging) ou o índice de Gini (para árvores de classificação bagging). No caso de árvores de regressão bagging, podemos registrar a quantidade total de redução do RSS devido a divisões em um determinado preditor, calculada a média de todas as árvores. Um valor alto indica um preditor importante. Da mesma forma, no contexto de árvores de classificação bagging, podemos somar a quantidade total de redução do índice de Gini devido a divisões em um determinado preditor, calculada a média de todas as árvores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importance\n",
    "feature_importances = np.mean([tree.feature_importances_ for tree in bagging.estimators_], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos dar uma olhada na importância de cada característica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8.9\n",
    "def make_figure_8_9():\n",
    "    \n",
    "    # Init\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "    ax.set_title('Figure 8.9: Feature Importance');\n",
    "\n",
    "    # Plot feature importance\n",
    "    h1 = pd.DataFrame({'Importance':feature_importances*100}, index=features)\n",
    "    h1 = h1.sort_values(by='Importance', axis=0, ascending=False)\n",
    "    h1.plot(kind='barh', color='r', ax=ax)\n",
    "    ax.set_xlabel('Variable Importance'); \n",
    "    plt.yticks(fontsize=14);\n",
    "    plt.gca().legend_ = None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figure_8_9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Florestas aleatórias\n",
    "\n",
    "\n",
    "Florestas aleatórias oferecem uma melhoria em relação às árvores ensacadas por meio de um pequeno ajuste que as descorrelaciona . Assim como no ensacamento, construímos uma série de árvores de decisão em amostras de treinamento bootstrapped. Mas, ao construir essas árvores de decisão, cada vez que uma divisão em uma árvore é considerada, uma amostra aleatória de\n",
    "preditores são escolhidos como candidatos divididos do conjunto completo de\n",
    "preditores. A divisão pode usar apenas um desses m preditores. Uma nova amostra de\n",
    "os preditores são obtidos em cada divisão e, normalmente, escolhemos\n",
    "— isto é, o número de preditores considerados em cada divisão é aproximadamente igual à raiz quadrada do número total de preditores\n",
    "\n",
    "Em outras palavras, ao construir uma floresta aleatória, em cada divisão na árvore, o algoritmo não tem permissão nem para considerar a maioria dos preditores disponíveis. Isso pode parecer loucura, mas tem uma justificativa inteligente. Suponha que haja um preditor muito forte no conjunto de dados, juntamente com vários outros preditores moderadamente fortes. Então, na coleção de árvores ensacadas, a maioria ou todas as árvores usarão esse preditor forte na divisão superior. Consequentemente, todas as árvores ensacadas parecerão bastante semelhantes entre si. Portanto, as previsões das árvores ensacadas serão altamente correlacionadas. Infelizmente, a média de muitas quantidades altamente correlacionadas não leva a uma redução tão grande na variância quanto a média de muitas quantidades não correlacionadas. Em particular, isso significa que o ensacamento não levará a uma redução substancial na variância em uma única árvore nesse cenário.\n",
    "\n",
    "Florestas aleatórias superam esse problema forçando cada divisão a considerar apenas um subconjunto dos preditores .\n",
    "\n",
    "Vamos dividir os dados em 2 e calcular o teste e a estimativa\n",
    ", tanto para florestas quanto para árvores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Init (takes a lot of time with J=30)\n",
    "params = range(2,50)\n",
    "forest_scores = np.zeros((len(params),2))\n",
    "J = 30\n",
    "\n",
    "# Loop over parameters\n",
    "for i, k in enumerate(params):\n",
    "    print(\"Computing k=%1.0f\" % k, end =\"\")\n",
    "    \n",
    "    # Repeat J \n",
    "    temp_scores = np.zeros((J,2))\n",
    "    for j in range(J):\n",
    "        X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.5, random_state=j)\n",
    "        forest = RandomForestClassifier(n_estimators=k, oob_score=True, max_features=\"sqrt\")\n",
    "        forest.fit(X2_train,y2_train)\n",
    "        temp_scores[j,0] = forest.score(X2_test, y2_test)\n",
    "        temp_scores[j,1] = forest.oob_score_\n",
    "        \n",
    "    # Average\n",
    "    forest_scores[i,:] = np.mean(temp_scores, axis=0)\n",
    "    print(\"\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8.8\n",
    "def make_figure_8_8():\n",
    "    \n",
    "    # Init\n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,6))\n",
    "    ax.set_title('Figure 8.8');\n",
    "\n",
    "    # Plot scores\n",
    "    ax.plot(params, bagging_scores);\n",
    "    ax.plot(params, forest_scores);\n",
    "    ax.legend(['Test - Bagging','OOB - Bagging', 'Test - Forest','OOB - Forest']);\n",
    "    ax.set_xlabel('Number of Trees'); ax.set_ylabel('R^2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figure_8_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto ao ensacamento, podemos traçar a importância do recurso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new figure 2\n",
    "def make_new_figure_2():\n",
    "    \n",
    "    # Init\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,7))\n",
    "\n",
    "    # Plot feature importance - Bagging\n",
    "    h1 = pd.DataFrame({'Importance':feature_importances*100}, index=features)\n",
    "    h1 = h1.sort_values(by='Importance', axis=0, ascending=False)\n",
    "    h1.plot(kind='barh', color='r', ax=ax1)\n",
    "    ax1.set_xlabel('Variable Importance'); \n",
    "    ax1.set_title('Tree Bagging')\n",
    "\n",
    "    # Plot feature importance\n",
    "    h2 = pd.DataFrame({'Importance':forest.feature_importances_*100}, index=features)\n",
    "    h2 = h2.sort_values(by='Importance', axis=0, ascending=False)\n",
    "    h2.plot(kind='barh', color='r', ax=ax2)\n",
    "    ax2.set_title('Random Forest')\n",
    "\n",
    "    # All plots\n",
    "    for ax in fig.axes:\n",
    "        ax.set_xlabel('Variable Importance'); \n",
    "        ax.legend([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_figure_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na figura, observamos que a classificação de importância das variáveis ​​é semelhante à do bagging e das florestas aleatórias, mas há diferenças significativas.\n",
    "\n",
    "Agora, vamos analisar a importância das florestas aleatórias usando o Khanconjunto de dados genéticos. Este conjunto de dados tem a peculiaridade de possuir um grande número de características e pouquíssimas observações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "gene = pd.read_csv('data/Khan.csv')\n",
    "print(len(gene))\n",
    "gene.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 linhas × 2309 colunas\n",
    "\n",
    "O conjunto de dados tem 83 linhas e 2309 colunas.\n",
    "\n",
    "Como é um conjunto de dados muito amplo , selecionar os recursos corretos é crucial.\n",
    "\n",
    "Observe também que não podemos executar regressão linear neste conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset size\n",
    "gene_small = gene.iloc[:,0:202]\n",
    "X = gene_small.iloc[:,1:]\n",
    "y = gene_small.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora fazer uma validação cruzada sobre o número de árvores e o número máximo de recursos considerados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init (takes a lot of time with J=30)\n",
    "params = range(50,150,10)\n",
    "m_scores = np.zeros((len(params),3))\n",
    "p = np.shape(X)[1]\n",
    "J = 30;\n",
    "\n",
    "# Loop over parameters\n",
    "for i, k in enumerate(params):\n",
    "    \n",
    "    # Array of features\n",
    "    ms = [round(p/2), round(np.sqrt(p)), round(np.log(p))]\n",
    "    \n",
    "    # Repeat L times\n",
    "    temp_scores = np.zeros((J,3))\n",
    "    for j in range(J):\n",
    "        print(\"Computing k=%1.0f (iter=%1.0f)\" % (k,j+1), end =\"\")\n",
    "    \n",
    "        # Loop over values of m\n",
    "        for index, m in enumerate(ms):\n",
    "            forest = RandomForestClassifier(n_estimators=k, max_features=m, oob_score=True)\n",
    "            forest.fit(X, y)\n",
    "            temp_scores[j,index] = forest.oob_score_\n",
    "        print(\"\", end=\"\\r\")\n",
    "            \n",
    "    # Average\n",
    "    m_scores[i,:] = np.mean(temp_scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8.10\n",
    "def make_figure_8_10():\n",
    "    \n",
    "    # Init\n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,6))\n",
    "    ax.set_title('Figure 8.10');\n",
    "\n",
    "    # Plot scores\n",
    "    ax.plot(params, m_scores);\n",
    "    ax.legend(['m=p/2','m=sqrt(p)','m=log(p)']);\n",
    "    ax.set_xlabel('Number of Trees'); ax.set_ylabel('Test Classification Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " make_figure_8_10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que as melhores pontuações são alcançadas com poucos recursos e muitas árvores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting (Impulsionando)\n",
    "\n",
    "Assim como o bagging, o boosting é uma abordagem geral que pode ser aplicada a muitos métodos de aprendizado estatístico para regressão ou classificação. Aqui, restringimos nossa discussão sobre boosting ao contexto de árvores de decisão.\n",
    "\n",
    "O boosting funciona de forma semelhante ao bagging, exceto que as árvores são cultivadas sequencialmente: cada árvore é cultivada usando informações de árvores cultivadas anteriormente. O boosting não envolve amostragem bootstrap; em vez disso, cada árvore é ajustada a uma versão modificada do conjunto de dados original.\n",
    "\n",
    "Qual é a ideia por trás desse procedimento? Dado o modelo atual, ajustamos uma árvore de decisão aos resíduos do modelo. Ou seja, ajustamos uma árvore usando os resíduos atuais , em vez do resultado.\n",
    ", como a resposta. Em seguida, adicionamos essa nova árvore de decisão à função ajustada para atualizar os resíduos. Cada uma dessas árvores pode ser bastante pequena, com apenas alguns nós terminais, determinados pelo parâmetro\n",
    "no algoritmo. Ajustando pequenas árvores aos resíduos, melhoramos lentamente\n",
    "em áreas onde não apresenta bom desempenho . O parâmetro de encolhimento λ retarda ainda mais o processo, permitindo que mais árvores, com formatos diferentes, ataquem os resíduos. Em geral, abordagens de aprendizado estatístico que aprendem lentamente tendem a apresentar bom desempenho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo\n",
    "O algoritmo de reforço funciona da seguinte maneira:\n",
    "\n",
    "Definir\n",
    "e\n",
    "para todos\n",
    "no conjunto de treinamento.\n",
    "\n",
    "Para\n",
    "repita:\n",
    "\n",
    "a. Encaixe uma árvore\n",
    "com\n",
    "divisões (\n",
    "nós terminais) para os dados de treinamento\n",
    ".\n",
    "\n",
    "b. Atualização\n",
    "adicionando uma versão reduzida da nova árvore:\n",
    "\n",
    "c. Atualizar os resíduos\n",
    "\n",
    "Saída do modelo impulsionado\n",
    "\n",
    "O Boosting tem três parâmetros de ajuste:\n",
    "\n",
    "O número de árvores \n",
    "O parâmetro de encolhimento \n",
    ". Isso controla a taxa de aprendizado do boosting.\n",
    "O número de divisões em cada árvore \n",
    ", que controla a complexidade do conjunto reforçado. Frequentemente, d = 1 funciona bem, caso em que cada árvore é um toco, consistindo em uma única divisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init , oob_score=True\n",
    "params = range(50,150,10)\n",
    "boost_scores = np.zeros((len(params),3))\n",
    "p = np.shape(X)[1]\n",
    "J = 30\n",
    "\n",
    "# Loop over parameters\n",
    "for i, k in enumerate(params):\n",
    "    \n",
    "    # Repeat L times\n",
    "    temp_scores = np.zeros((J,3))\n",
    "    for j in range(J):\n",
    "        print(\"Computing k=%1.0f (iter=%1.0f)\" % (k,j+1), end =\"\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=j)\n",
    "    \n",
    "        # First score: random forest\n",
    "        forest = RandomForestClassifier(n_estimators=k, max_features=\"sqrt\")\n",
    "        forest.fit(X_train, y_train)\n",
    "        temp_scores[j,0] = forest.score(X_test, y_test)\n",
    "\n",
    "        # Second score: boosting with 1-split trees\n",
    "        boost1 = GradientBoostingClassifier(learning_rate=0.01, max_depth=1, n_estimators=k, max_features=\"sqrt\")\n",
    "        boost1.fit(X_train, y_train)\n",
    "        temp_scores[j,1] = boost1.score(X_test, y_test)\n",
    "\n",
    "        # Third score: boosting with 1-split trees\n",
    "        boost2 = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=k, max_features=\"sqrt\")\n",
    "        boost2.fit(X_train, y_train)\n",
    "        temp_scores[j,2] = boost2.score(X_test, y_test)\n",
    "        print(\"\", end=\"\\r\")\n",
    "    \n",
    "    # Average\n",
    "    boost_scores[i,:] = np.mean(temp_scores, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos comparar boosting e floresta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
