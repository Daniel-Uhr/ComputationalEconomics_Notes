{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervisioned Learning (Aprendizado Não Supervisionado)\n",
    "\n",
    "Prof. Daniel de Abreu Pereira Uhr\n",
    "\n",
    "### Conteúdo\n",
    "* Introdução\n",
    "* Cross-Validation (Validação Cruzada)\n",
    "  * The Validation Set Approach (A Abordagem do Conjunto de Validação)\n",
    "  * Leave-One-Out Cross-Validation (Validação Cruzada Leave-One-Out)\n",
    "  * k-Fold Cross-Validation (Validação Cruzada k-Fold)\n",
    "* The Bootstrap\n",
    "\n",
    "### Referências\n",
    "\n",
    "* [An Introduction to Statistical Learning](https://www.statlearning.com/) (ISL) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani\n",
    "  * Capítulo 5***\n",
    "* [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/) (ESL) by Trevor Hastie, Robert Tibshirani and Jerome Friedman : \n",
    "  * Capítulo 7\n",
    "\n",
    "***Disclaimer:*** *O material apresentado aqui é uma adaptação do material de aula do Prof. Daniel de Abreu Pereira Uhr, e não deve ser utilizado para fins comerciais. O material é disponibilizado para fins educacionais e de pesquisa, e não deve ser reproduzido sem a devida autorização do autor. Este material pode conter erros e imprecisões. O autor não se responsabiliza por quaisquer danos ou prejuízos decorrentes do uso deste material. O uso deste material é de responsabilidade exclusiva do usuário. Caso você encontre erros ou imprecisões neste material, por favor, entre em contato com o autor para que possam ser corrigidos. O autor agradece qualquer feedback ou sugestão de melhoria.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Sklean\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, cut_tree\n",
    "\n",
    "# Import matplotlib for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set global parameters\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['figure.titlesize'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "\n",
    "# Figure 10.1 a\n",
    "def make_figure_10_1a(df_dim2, df_weights):\n",
    "\n",
    "    # Init\n",
    "    fig, ax1 = plt.subplots(figsize=(8,8))\n",
    "    ax1.set_title('Figure 10.1');\n",
    "\n",
    "    # Plot Principal Components 1 and 2\n",
    "    for i in df_dim2.index:\n",
    "        ax1.annotate(i, (df_dim2.PC1.loc[i], -df_dim2.PC2.loc[i]), ha='center', fontsize=14)\n",
    "\n",
    "    # Plot reference lines\n",
    "    m = np.max(np.abs(df_dim2.values))*1.2\n",
    "    ax1.hlines(0,-m,m, linestyles='dotted', colors='grey')\n",
    "    ax1.vlines(0,-m,m, linestyles='dotted', colors='grey')\n",
    "    ax1.set_xlabel('First Principal Component')\n",
    "    ax1.set_ylabel('Second Principal Component')\n",
    "    ax1.set_xlim(-m,m); ax1.set_ylim(-m,m)\n",
    "\n",
    "    # Plot Principal Component loading vectors, using a second y-axis.\n",
    "    ax1b = ax1.twinx().twiny() \n",
    "    ax1b.set_ylim(-1,1); ax1b.set_xlim(-1,1)\n",
    "    for i in df_weights[['PC1', 'PC2']].index:\n",
    "        ax1b.annotate(i, (df_weights.PC1.loc[i]*1.05, -df_weights.PC2.loc[i]*1.05), color='orange', fontsize=16)\n",
    "        ax1b.arrow(0,0,df_weights.PC1[i], -df_weights.PC2[i], color='orange', lw=2)\n",
    "        \n",
    "        \n",
    "# Figure 10.1 b\n",
    "def make_figure_10_1b(df_dim2, df_dim2_u, df_weights, df_weights_u):\n",
    "\n",
    "    # Init\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(18,9))\n",
    "\n",
    "    # Scaled PCA\n",
    "    for i in df_dim2.index:\n",
    "        ax1.annotate(i, (df_dim2.PC1.loc[i], -df_dim2.PC2.loc[i]), ha='center', fontsize=14)\n",
    "    ax1b = ax1.twinx().twiny() \n",
    "    ax1b.set_ylim(-1,1); ax1b.set_xlim(-1,1)\n",
    "    for i in df_weights[['PC1', 'PC2']].index:\n",
    "        ax1b.annotate(i, (df_weights.PC1.loc[i]*1.05, -df_weights.PC2.loc[i]*1.05), color='orange', fontsize=16)\n",
    "        ax1b.arrow(0,0,df_weights.PC1[i], -df_weights.PC2[i], color='orange', lw=2)\n",
    "    ax1.set_title('Scaled')\n",
    "\n",
    "    # Unscaled PCA\n",
    "    for i in df_dim2_u.index:\n",
    "        ax2.annotate(i, (df_dim2_u.PC1.loc[i], -df_dim2_u.PC2.loc[i]), ha='center', fontsize=14)\n",
    "    ax2b = ax2.twinx().twiny() \n",
    "    ax2b.set_ylim(-1,1); ax2b.set_xlim(-1,1)\n",
    "    for i in df_weights_u[['PC1', 'PC2']].index:\n",
    "        ax2b.annotate(i, (df_weights_u.PC1.loc[i]*1.05, -df_weights_u.PC2.loc[i]*1.05), color='orange', fontsize=16)\n",
    "        ax2b.arrow(0,0,df_weights_u.PC1[i], -df_weights_u.PC2[i], color='orange', lw=2)\n",
    "    ax2.set_title('Unscaled')\n",
    "\n",
    "    # Plot reference lines\n",
    "    for ax,df in zip((ax1,ax2), (df_dim2,df_dim2_u)):\n",
    "        m = np.max(np.abs(df.values))*1.2\n",
    "        ax.hlines(0,-m,m, linestyles='dotted', colors='grey')\n",
    "        ax.vlines(0,-m,m, linestyles='dotted', colors='grey')\n",
    "        ax.set_xlabel('First Principal Component')\n",
    "        ax.set_ylabel('Second Principal Component')\n",
    "        ax.set_xlim(-m,m); ax.set_ylim(-m,m)\n",
    "\n",
    "\n",
    "# Figure 10.2\n",
    "def make_figure_10_2(pca4):\n",
    "\n",
    "    # Init\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n",
    "    fig.suptitle('Figure 10.2');\n",
    "\n",
    "    # Relative \n",
    "    ax1.plot([1,2,3,4], pca4.explained_variance_ratio_)\n",
    "    ax1.set_ylabel('Prop. Variance Explained')\n",
    "    ax1.set_xlabel('Principal Component');\n",
    "\n",
    "    # Cumulative\n",
    "    ax2.plot([1,2,3,4], np.cumsum(pca4.explained_variance_ratio_))\n",
    "    ax2.set_ylabel('Cumulative Variance Explained');\n",
    "    ax2.set_xlabel('Principal Component');\n",
    "\n",
    "\n",
    "# Figure new 1\n",
    "def make_new_figure_1(X):\n",
    "    \n",
    "    # Init\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    fig.suptitle(\"Baseline\")\n",
    "\n",
    "    # Plot\n",
    "    ax.scatter(X[:,0], X[:,1], s=50, alpha=0.5, c='k') \n",
    "    ax.set_xlabel('X0'); ax.set_ylabel('X1');\n",
    "    \n",
    "\n",
    "# Figure new 2\n",
    "def make_new_figure_2(X, clusters0):\n",
    "    \n",
    "    # Init\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    fig.suptitle(\"Random assignment\")\n",
    "\n",
    "    # Plot\n",
    "    ax.scatter(X[clusters0==0,0], X[clusters0==0,1], s=50, alpha=0.5) \n",
    "    ax.scatter(X[clusters0==1,0], X[clusters0==1,1], s=50, alpha=0.5)\n",
    "    ax.set_xlabel('X0'); ax.set_ylabel('X1');\n",
    "\n",
    "\n",
    "# Plot assignment\n",
    "def plot_assignment(X, centroids, clusters, d, i):\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    fig.suptitle(\"Iteration %.0f: inertia=%.1f\" % (i,d))\n",
    "\n",
    "    # Plot\n",
    "    ax.clear()\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color'];\n",
    "    K = np.size(centroids,0)\n",
    "    for k in range(K):\n",
    "        ax.scatter(X[clusters==k,0], X[clusters==k,1], s=50, c=colors[k], alpha=0.5) \n",
    "        ax.scatter(centroids[k,0], centroids[k,1], marker = '*', s=300, color=colors[k])\n",
    "        ax.set_xlabel('X0'); ax.set_ylabel('X1');\n",
    "    \n",
    "    # Show\n",
    "    plt.show();\n",
    "    \n",
    "# Figure new 3\n",
    "def make_new_figure_3(d):\n",
    "    \n",
    "    # Init\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "\n",
    "    # calculate full dendrogram\n",
    "    plt.xlabel('sample index')\n",
    "    plt.ylabel('distance')\n",
    "    d\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Figure new 4\n",
    "def make_new_figure_4(linkages, titles):\n",
    "    \n",
    "    # Init\n",
    "    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(15,6))\n",
    "\n",
    "    # Plot\n",
    "    for linkage, t, ax in zip(linkages, titles, (ax1,ax2,ax3)):\n",
    "        dendrogram(linkage, ax=ax)\n",
    "        ax.set_title(t)\n",
    "        \n",
    "        \n",
    "def get_cov_ellipse(distr, nstd, **kwargs):\n",
    "    \"\"\"\n",
    "    Return a matplotlib Ellipse patch representing a standard distribution around the mean\n",
    "    \"\"\"\n",
    "\n",
    "    # Find and sort eigenvalues and eigenvectors into descending order\n",
    "    eigvals, eigvecs = np.linalg.eigh(distr.cov)\n",
    "    order = eigvals.argsort()[::-1]\n",
    "    eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n",
    "\n",
    "    # The anti-clockwise angle to rotate our ellipse by \n",
    "    vx, vy = eigvecs[:,0][0], eigvecs[:,0][1]\n",
    "    theta = np.arctan2(vy, vx)\n",
    "\n",
    "    # Width and height of ellipse to draw\n",
    "    width, height = 2 * nstd * np.sqrt(eigvals)\n",
    "    return Ellipse(xy=distr.mean, width=width, height=height,\n",
    "                   angle=np.degrees(theta), **kwargs)\n",
    "\n",
    "\n",
    "# Plot assignment\n",
    "def plot_assignment_gmm(X, clusters, distr, i, logL):\n",
    "    clear_output(wait=True)\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    fig.suptitle(f\"Iteration {i}, logL={logL:.2}\")\n",
    "\n",
    "    # Plot\n",
    "    ax.clear()\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color'];\n",
    "    K = len(distr)\n",
    "    for k in range(K):\n",
    "        ax.scatter(X[clusters==k,0], X[clusters==k,1], s=50, c=colors[k], alpha=0.5) \n",
    "        ax.scatter(distr[k].mean[0], distr[k].mean[1], marker = '*', s=300, color=colors[k])\n",
    "        for i in [0.5, 1, 2]:\n",
    "            ax.add_artist(get_cov_ellipse(distr[k], nstd=i, color=colors[k], alpha=0.05))\n",
    "        ax.set_xlabel('X0'); ax.set_ylabel('X1');\n",
    "    \n",
    "    # Show\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizagem supervisionada vs. não supervisionada\n",
    "\n",
    "A diferença entre aprendizagem supervisionada e aprendizagem não supervisionada é que no primeiro caso temos uma variável\n",
    "que queremos prever, dado um conjunto de variáveis\n",
    ".\n",
    "\n",
    "Na aprendizagem não supervisionada não estamos interessados ​​em previsão, porque não temos uma variável de resposta associada\n",
    ". Em vez disso, o objetivo é descobrir propriedades interessantes sobre as medições em\n",
    ".\n",
    "\n",
    "As questões que normalmente nos interessam são:\n",
    "\n",
    "Agrupamento\n",
    "Redução de dimensionalidade\n",
    "Em geral, o aprendizado não supervisionado pode ser visto como uma extensão da análise exploratória de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redução da dimensionalidade\n",
    "Trabalhar em espaços de alta dimensão pode ser indesejável por vários motivos; dados brutos geralmente são esparsos como consequência da maldição da dimensionalidade, e analisar os dados geralmente é computacionalmente intratável (difícil de controlar ou lidar).\n",
    "\n",
    "A redução da dimensionalidade também pode ser útil para plotar dados de alta dimensionalidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupamento\n",
    "Agrupamento refere-se a um conjunto muito amplo de técnicas para encontrar subgrupos, ou clusters, em um conjunto de dados. Quando agrupamos as observações de um conjunto de dados, buscamos particioná-las em grupos distintos, de modo que as observações dentro de cada grupo sejam bastante semelhantes entre si, enquanto as observações em grupos diferentes sejam bastante diferentes entre si.\n",
    "\n",
    "Nesta seção, nos concentramos nos seguintes algoritmos:\n",
    "\n",
    "Agrupamento de K-means\n",
    "Agrupamento hierárquico\n",
    "Modelos de Mistura Gaussiana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise de Componentes Principais\n",
    "\n",
    "Suponha que desejamos visualizar\n",
    "observações com medições em um conjunto de\n",
    "características,\n",
    ", como parte de uma análise exploratória de dados.\n",
    "\n",
    "Poderíamos fazer isso examinando gráficos de dispersão bidimensionais dos dados, cada um contendo as medições das n observações em duas das características. No entanto, existem\n",
    "tais gráficos de dispersão; por exemplo, com\n",
    "há\n",
    "tramas!\n",
    "\n",
    "A ACP fornece uma ferramenta para fazer exatamente isso. Ela encontra uma representação de baixa dimensão de um conjunto de dados que contém o máximo possível da variação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro Componente Principal\n",
    "\n",
    "O primeiro componente principal de um conjunto de recursos\n",
    "é a combinação linear normalizada das características\n",
    "\n",
    "\n",
    "que tem a maior variância.\n",
    "\n",
    "Por normalizado, queremos dizer que\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computação PCA\n",
    "Em outras palavras, o primeiro vetor de carga do componente principal resolve o problema de otimização\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "O objetivo que estamos maximizando é apenas a variância da amostra do\n",
    "valores de\n",
    ".\n",
    "\n",
    "Após o primeiro componente principal\n",
    "das características foi determinado, podemos encontrar o segundo componente principal\n",
    ". O segundo componente principal é a combinação linear de\n",
    "que tem variância máxima de todas as combinações lineares que não são correlacionadas com\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo\n",
    "\n",
    "Ilustramos o uso de PCA no USArrestsconjunto de dados.\n",
    "\n",
    "Para cada um dos 50 estados dos Estados Unidos, o conjunto de dados contém o número de prisões por\n",
    "residentes para cada um dos três crimes: Assault, Murder, e Rape.Também registramos a porcentagem da população em cada estado que vive em áreas urbanas, UrbanPop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crime data\n",
    "df = pd.read_csv('data/USArrests.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionamento de dados\n",
    "\n",
    "Para tornar todas as características comparáveis, primeiro precisamos escaloná-las. Nesse caso, usamos a sklearn.preprocessing.scale()função para normalizar cada variável para que tenha média zero e variância unitária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "X_scaled = pd.DataFrame(scale(df), index=df.index, columns=df.columns).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eremos mais tarde quais são as implicações práticas da (não) escala.\n",
    "\n",
    "Montagem\n",
    "Vamos ajustar o PCA com 2 componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with 2 components\n",
    "pca2 = PCA(n_components=2).fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights\n",
    "weights = pca2.components_.T\n",
    "df_weights = pd.DataFrame(weights, index=df.columns, columns=['PC1', 'PC2'])\n",
    "df_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projetando os dados\n",
    "Como são os dados transformados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X to get the principal components\n",
    "X_dim2 = pca2.transform(X_scaled)\n",
    "df_dim2 = pd.DataFrame(X_dim2, columns=['PC1', 'PC2'], index=df.index)\n",
    "df_dim2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização\n",
    "\n",
    "A vantagem do PCA é que ele nos permite ver a variação em dimensões menores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figure_10_1a(df_dim2, df_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA e Análise Espectral\n",
    "Caso você não tenha notado, calcular os componentes principais é equivalente a calcular os autovetores da matriz de projeto\n",
    ", ou seja, a matriz de variância-covariância de\n",
    ". Na verdade, o que realizamos acima é uma decomposição da variância de\n",
    "em componentes ortogonais.\n",
    "\n",
    "O problema de maximização restrita acima pode ser reescrito em notação matricial como\n",
    "\n",
    "\n",
    "Que tem a seguinte representação dupla\n",
    "\n",
    "\n",
    "Se tomarmos as condições de primeira ordem\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "Definindo as derivadas como zero no ótimo, obtemos\n",
    "\n",
    " \n",
    "\n",
    "Por isso,\n",
    "é um autovetor da matriz de covariância\n",
    ", e o vetor maximizador será aquele associado ao maior autovalor \n",
    ".\n",
    "\n",
    "Autovalores e autovetores\n",
    "Agora podemos verificar novamente usando numpyo pacote de álgebra linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenval, eigenvec = np.linalg.eig(X_scaled.T @ X_scaled)\n",
    "data = np.concatenate((eigenvec,eigenval.reshape(1,-1)))\n",
    "idx = list(df.columns) + ['Eigenvalue']\n",
    "df_eigen = pd.DataFrame(data, index=idx, columns=['PC1', 'PC2','PC3','PC4'])\n",
    "\n",
    "df_eigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decomposição espectral da variância de\n",
    "gera um conjunto de vetores ortogonais (autovetores) com diferentes magnitudes (autovalores). Os autovalores nos indicam a quantidade de variância dos dados naquela direção.\n",
    "\n",
    "Se combinarmos os autovetores, formamos uma matriz de projeção\n",
    "que podemos usar para transformar as variáveis ​​originais:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = X_scaled @ eigenvec\n",
    "df_transformed = pd.DataFrame(X_transformed, index=df.index, columns=['PC1', 'PC2','PC3','PC4'])\n",
    "\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é exatamente o conjunto de dados que obtivemos antes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalando as Variáveis\n",
    "\n",
    "Os resultados obtidos ao realizarmos a ACP também dependerão de as variáveis ​​terem sido escalonadas individualmente. De fato, a variância de uma variável depende de sua magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables variance\n",
    "df.var(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequentemente, se executarmos a ACP nas variáveis ​​não escalonadas, o primeiro vetor de carga do componente principal terá uma carga muito grande para Assault, já que essa variável tem de longe a maior variância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with unscaled varaibles\n",
    "X = df.values\n",
    "pca2_u = PCA(n_components=2).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights\n",
    "weights_u = pca2_u.components_.T\n",
    "df_weights_u = pd.DataFrame(weights_u, index=df.columns, columns=['PC1', 'PC2'])\n",
    "df_weights_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X to get the principal components\n",
    "X_dim2_u = pca2_u.transform(X)\n",
    "df_dim2_u = pd.DataFrame(X_dim2_u, columns=['PC1', 'PC2'], index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotagem\n",
    "Podemos comparar as representações dimensionais inferiores com e sem escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figure_10_1b(df_dim2, df_dim2_u, df_weights, df_weights_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme previsto, o primeiro vetor de carga de componentes principais coloca quase todo o seu peso em Assault, enquanto o segundo vetor de carga de componentes principais coloca quase todo o seu peso em UrpanPop. Comparando isso com o gráfico da esquerda, vemos que a escala de fato tem um efeito substancial nos resultados obtidos. No entanto, esse resultado é simplesmente uma consequência das escalas em que as variáveis ​​foram medidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Proporção de Variância Explicada\n",
    "Podemos agora fazer uma pergunta natural: quanta informação em um determinado conjunto de dados é perdida ao projetar as observações nos primeiros componentes principais? Ou seja, quanta da variância nos dados não está contida nos primeiros componentes principais? De forma mais geral, estamos interessados ​​em saber a proporção da variância explicada (PVE) por cada componente principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four components\n",
    "pca4 = PCA(n_components=4).fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance of the four principal components\n",
    "pca4.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretação\n",
    "Podemos calculá-lo em porcentagem da variância total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a percentage of the total variance\n",
    "pca4.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Arrestconjunto de dados, o primeiro componente principal explica\n",
    "da variância nos dados, e o próximo componente principal explica\n",
    "da variância. Juntos, os dois primeiros componentes principais explicam quase\n",
    "da variância dos dados, e os dois últimos componentes principais explicam apenas\n",
    "da variância."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotagem\n",
    "Podemos traçar em um gráfico a porcentagem da variância explicada, em relação ao número de componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figure_10_2(pca4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantos componentes principais?\n",
    "\n",
    "\n",
    "Em geral, um\n",
    "matriz de dados\n",
    "tem\n",
    "componentes principais distintos. No entanto, normalmente não nos interessamos por todos eles; em vez disso, gostaríamos de usar apenas os primeiros componentes principais para visualizar ou interpretar os dados.\n",
    "\n",
    "Normalmente, decidimos o número de componentes principais necessários para visualizar os dados examinando um gráfico de declive .\n",
    "\n",
    "Entretanto, não há uma maneira objetiva bem aceita de decidir quantos componentes principais são suficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupamento de K-Means\n",
    "A ideia por trás do agrupamento K-means é que um bom agrupamento é aquele em que a variação dentro do agrupamento é a menor possível. Portanto, queremos resolver o problema\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "onde\n",
    "é um cluster e\n",
    "é uma medida da quantidade pela qual as observações dentro de um cluster diferem umas das outras.\n",
    "\n",
    "Existem muitas maneiras possíveis de definir este conceito, mas de longe a escolha mais comum envolve a distância euclidiana ao quadrado . Ou seja, definimos\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "onde\n",
    "denota o número de observações no\n",
    "conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo\n",
    "Atribuir um número aleatoriamente, de\n",
    "para\n",
    ", para cada uma das observações. Elas servem como atribuições iniciais de cluster para as observações.\n",
    "\n",
    "Repita até que as atribuições do cluster parem de mudar:\n",
    "\n",
    "a) Para cada um dos\n",
    "clusters, calcule o centróide do cluster. O centróide do cluster k é o vetor do\n",
    "recursos significam para as observações no\n",
    "conjunto.\n",
    "\n",
    "b) Atribua cada observação ao cluster cujo centroide é o mais próximo (onde o mais próximo é definido usando a distância euclidiana).\n",
    "\n",
    "Gerar os dados\n",
    "Primeiro, geramos um conjunto de dados bidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "np.random.seed(123)\n",
    "X = np.random.randn(50,2)\n",
    "X[0:25, 0] = X[0:25, 0] + 3\n",
    "X[0:25, 1] = X[0:25, 1] - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_figure_1(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 1: atribuição aleatória\n",
    "Agora vamos atribuir os dados aleatoriamente a dois clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init clusters\n",
    "K = 2\n",
    "clusters0 = np.random.randint(K,size=(np.size(X,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_new_figure_2(X, clusters0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 2: estimar distribuições\n",
    "O que são os novos centróides?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new centroids\n",
    "def compute_new_centroids(X, clusters):\n",
    "    K = len(np.unique(clusters))\n",
    "    centroids = np.zeros((K,np.size(X,1)))\n",
    "    for k in range(K):\n",
    "        if sum(clusters==k)>0:\n",
    "            centroids[k,:] = np.mean(X[clusters==k,:], axis=0)\n",
    "        else:\n",
    "            centroids[k,:] = np.mean(X, axis=0)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print\n",
    "centroids0 = compute_new_centroids(X, clusters0)\n",
    "print(centroids0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traçando os centróides\n",
    "Vamos adicionar os centroides ao gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plot_assignment(X, centroids0, clusters0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 3: atribuir dados aos clusters\n",
    "Agora podemos atribuir os dados aos clusters, de acordo com o centróide mais próximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X to clusters\n",
    "def assign_to_cluster(X, centroids):\n",
    "    K = np.size(centroids,0)\n",
    "    dist = np.zeros((np.size(X,0),K))\n",
    "    for k in range(K):\n",
    "        dist[:,k] = np.mean((X - centroids[k,:])**2, axis=1)\n",
    "    clusters = np.argmin(dist, axis=1)\n",
    "    \n",
    "    # Compute inertia\n",
    "    inertia = 0\n",
    "    for k in range(K):\n",
    "        if sum(clusters==k)>0:\n",
    "            inertia += np.sum((X[clusters==k,:] - centroids[k,:])**2)\n",
    "    return clusters, inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando dados atribuídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster assignment\n",
    "[clusters1,d] = assign_to_cluster(X, centroids0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plot_assignment(X, centroids0, clusters1, d, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo completo\n",
    "Agora temos todos os componentes para prosseguir iterativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_manual(X, K):\n",
    "\n",
    "    # Init\n",
    "    i = 0\n",
    "    d0 = 1e4\n",
    "    d1 = 1e5\n",
    "    clusters = np.random.randint(K,size=(np.size(X,0)))\n",
    "\n",
    "    # Iterate until convergence\n",
    "    while np.abs(d0-d1) > 1e-10:\n",
    "        d1 = d0\n",
    "        centroids = compute_new_centroids(X, clusters)\n",
    "        [clusters, d0] = assign_to_cluster(X, centroids)\n",
    "        plot_assignment(X, centroids, clusters, d0, i)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando o agrupamento k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "kmeans_manual(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, as observações podem ser facilmente plotadas porque são bidimensionais. Se houvesse mais de duas variáveis, poderíamos realizar a ACP e plotar os dois primeiros vetores de pontuação dos componentes principais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais clusters\n",
    "No exemplo anterior, sabíamos que realmente havia dois clusters porque geramos os dados. No entanto, para dados reais, em geral, não sabemos o número real de clusters. Poderíamos ter realizado o agrupamento K-means neste exemplo com K = 3. Se fizermos isso, o agrupamento K-means dividirá os dois clusters \"reais\", já que não possui informações sobre eles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=3\n",
    "kmeans_manual(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pacote Sklearn\n",
    "A função automatizada em sklearnpersorm\n",
    "- significa que o agrupamento é KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKlearn algorithm\n",
    "km1 = KMeans(n_clusters=3, n_init=1, random_state=1)\n",
    "km1.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans(n_clusters=3, n_init=1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotagem\n",
    "Podemos plotar a atribuição gerada pela KMeansfunção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plot_assignment(X, km1.cluster_centers_, km1.labels_, km1.inertia_, km1.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, os resultados são diferentes nos dois algoritmos? Por quê?\n",
    "-means é suscetível aos valores iniciais. Uma maneira de resolver esse problema é executar o algoritmo várias vezes e reportar apenas os melhores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atribuição Inicial\n",
    "Para executar a Kmeans()função em Python com múltiplas atribuições iniciais de cluster, usamos o n_initargumento (padrão: 10). Se um valor n_initmaior que um for usado, o agrupamento K-means será realizado usando múltiplas atribuições aleatórias, e a Kmeans()função reportará apenas os melhores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 runs\n",
    "km_30run = KMeans(n_clusters=3, n_init=30, random_state=1).fit(X)\n",
    "plot_assignment(X, km_30run.cluster_centers_, km_30run.labels_, km_30run.inertia_, km_30run.n_iter_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melhores Práticas\n",
    "Geralmente, é recomendável sempre executar o agrupamento K-means com um valor grande de n_init, como 20 ou 50, para evitar ficar preso em um ótimo local indesejado.\n",
    "\n",
    "Ao realizar o agrupamento K-means, além de usar múltiplas atribuições iniciais de clusters, também é importante definir uma semente aleatória usando o random_stateparâmetro. Dessa forma, as atribuições iniciais de clusters podem ser replicadas e a saída do K-means será totalmente reproduzível.\n",
    "\n",
    "Agrupamento hierárquico\n",
    "Uma desvantagem potencial do agrupamento K-means é que ele exige que pré-especificemos o número de agrupamentos\n",
    ".\n",
    "\n",
    "O agrupamento hierárquico é uma abordagem alternativa que não exige que nos comprometamos com uma escolha específica de\n",
    ".\n",
    "\n",
    "O Dendograma\n",
    "O agrupamento hierárquico tem uma vantagem adicional sobre o agrupamento K-means, pois resulta em uma representação atraente baseada em árvore das observações, chamada dendrograma ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dendrogram(\n",
    "        linkage(X, \"complete\"),\n",
    "        leaf_rotation=90.,  # rotates the x axis labels\n",
    "        leaf_font_size=8.,  # font size for the x axis labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretação\n",
    "Cada folha do dendrograma representa uma observação.\n",
    "\n",
    "À medida que subimos na árvore, algumas folhas começam a se fundir em galhos. Isso corresponde a observações semelhantes entre si. À medida que subimos na árvore, os próprios galhos se fundem, seja com folhas ou com outros galhos. Quanto mais cedo (na parte inferior da árvore) as fusões ocorrerem, mais semelhantes serão os grupos de observações entre si.\n",
    "\n",
    "Podemos usar o dendograma para entender o quão semelhantes são duas observações: podemos procurar o ponto na árvore onde os ramos que contêm essas duas observações se fundem pela primeira vez. A altura dessa fusão, medida no eixo vertical, indica o quão diferentes são as duas observações. Assim, observações que se fundem na base da árvore são bastante semelhantes entre si, enquanto observações que se fundem perto do topo da árvore tendem a ser bastante diferentes.\n",
    "\n",
    "O termo hierárquico se refere ao fato de que os clusters obtidos pelo corte do dendrograma em uma determinada altura são necessariamente aninhados dentro dos clusters obtidos pelo corte do dendrograma em qualquer altura maior.\n",
    "\n",
    "O Algoritmo de Agrupamento Hierárquico\n",
    "Comece com\n",
    "observações e uma medida (como a distância euclidiana) de todos os\n",
    "Dissimilaridades em pares. Trate cada observação como um cluster separado.\n",
    "\n",
    "Para\n",
    "\n",
    "a) Examine todas as diferenças entre pares de grupos entre os\n",
    "clusters e identifique o par de clusters que são menos dissimilares (ou seja, mais semelhantes). Funda esses dois clusters. A dissimilaridade entre esses dois clusters indica a altura no dendrograma em que a fusão deve ser colocada.\n",
    "\n",
    "b) Calcule as novas diferenças entre pares de clusters entre os\n",
    "clusters restantes.\n",
    "\n",
    "A função de ligação\n",
    "Temos um conceito de dissimilaridade entre pares de observações, mas como definimos a dissimilaridade entre dois clusters se um ou ambos os clusters contêm múltiplas observações?\n",
    "\n",
    "O conceito de dissimilaridade entre um par de observações precisa ser estendido a um par de grupos de observações. Essa extensão é alcançada desenvolvendo a noção de ligação , que define a dissimilaridade entre dois grupos de observações.\n",
    "\n",
    "Ligações\n",
    "Os quatro tipos mais comuns de ligação são:\n",
    "\n",
    "Completo : Dissimilaridade máxima entre grupos. Calcule todas as dissimilaridades em pares entre as observações no grupo A e as observações no grupo B e registre a maior dessas dissimilaridades.\n",
    "Único : Dissimilaridade mínima entre grupos. Calcule todas as dissimilaridades em pares entre as observações no grupo A e as observações no grupo B e registre a menor dessas dissimilaridades.\n",
    "Média : Dissimilaridade média entre grupos. Calcule todas as dissimilaridades em pares entre as observações no grupo A e as observações no grupo B e registre a média dessas dissimilaridades.\n",
    "Centroide : Dissimilaridade entre o centroide do cluster A (um vetor médio de comprimento p) e o centroide do cluster B. A ligação do centroide pode resultar em inversões indesejáveis.\n",
    "Ligações médias, completas e simples são as mais populares entre os estatísticos. Ligações médias e completas são geralmente preferidas à ligação simples, pois tendem a produzir dendrogramas mais equilibrados. A ligação centroide é frequentemente usada em genômica, mas sofre de uma grande desvantagem: pode ocorrer uma inversão, na qual dois clusters se fundem a uma altura abaixo de qualquer um dos clusters individuais no dendrograma. Isso pode levar a dificuldades na visualização, bem como na interpretação do dendrograma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "linkages = [hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)]\n",
    "titles = ['Complete Linkage', 'Average Linkage', 'Single Linkage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_figure_4(linkages, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esses dados, tanto a vinculação completa quanto a média geralmente separam as observações em seus grupos corretos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos de Mistura Gaussiana\n",
    "Métodos de agrupamento, como agrupamento hierárquico e K-means, são baseados em heurísticas e dependem principalmente da descoberta de agrupamentos cujos membros são próximos uns dos outros, conforme medido diretamente com os dados (nenhum modelo de probabilidade envolvido).\n",
    "\n",
    "Os Modelos de Mistura Gaussiana pressupõem que os dados foram gerados por múltiplas distribuições gaussianas multivariadas. O objetivo do algoritmo é recuperar essas distribuições latentes.\n",
    "\n",
    "As vantagens em relação ao K-means são\n",
    "\n",
    "uma interpretação estrutural dos parâmetros\n",
    "gera automaticamente probabilidades de classe\n",
    "pode gerar grupos de observações que não estão necessariamente próximos uns dos outros\n",
    "Algoritmo\n",
    "Atribuir um número aleatoriamente, de\n",
    "para\n",
    ", para cada uma das observações. Elas servem como atribuições iniciais de cluster para as observações.\n",
    "\n",
    "Repita até que as atribuições do cluster parem de mudar:\n",
    "\n",
    "a) Para cada um dos\n",
    "clusters, calculamos sua média e variância. A principal diferença com o K-means é que também calculamos a matriz de variância.\n",
    "\n",
    "b) Atribua cada observação ao seu cluster mais provável.\n",
    "\n",
    "Conjunto de dados\n",
    "Vamos usar os mesmos dados que usamos para k-means, para uma comparação direta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_figure_1(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 1: atribuição aleatória\n",
    "Vamos também usar a mesma atribuição aleatória do algoritmo K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_figure_2(X, clusters0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 2: calcular distribuições\n",
    "Quais são as novas distribuições?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new centroids\n",
    "def compute_distributions(X, clusters):\n",
    "    K = len(np.unique(clusters))\n",
    "    distr = []\n",
    "    for k in range(K):\n",
    "        if sum(clusters==k)>0:\n",
    "            distr += [multivariate_normal(np.mean(X[clusters==k,:], axis=0), np.cov(X[clusters==k,:].T))]\n",
    "        else:\n",
    "            distr += [multivariate_normal(np.mean(X, axis=0), np.cov(X.T))]\n",
    "    return distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print\n",
    "distr0 = compute_distributions(X, clusters0)\n",
    "print(\"Mean of the first distribution: \\n\", distr0[0].mean)\n",
    "print(\"\\nVariance of the first distribution: \\n\", distr0[0].cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traçando as distribuições\n",
    "Vamos adicionar as distribuições ao gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plot_assignment_gmm(X, clusters0, distr0, i=0, logL=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilidade\n",
    "A principal diferença em relação ao K-means é que agora podemos calcular a probabilidade de cada observação pertencer a cada cluster. Essa é a probabilidade de cada observação ter sido gerada por uma das duas distribuições normais bivariadas. Essas probabilidades são chamadas de verossimilhanças ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first 5 likelihoods\n",
    "pdfs0 = np.stack([d.pdf(X) for d in distr0], axis=1)\n",
    "pdfs0[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Etapa 3: atribuir dados aos clusters\n",
    "Agora podemos atribuir os dados aos clusters, via máxima verossimilhança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X to clusters\n",
    "def assign_to_cluster_gmm(X, distr):\n",
    "    pdfs = np.stack([d.pdf(X) for d in distr], axis=1)\n",
    "    clusters = np.argmax(pdfs, axis=1)\n",
    "    log_likelihood = 0\n",
    "    for k, pdf in enumerate(pdfs):\n",
    "        log_likelihood += np.log(pdf[clusters[k]])\n",
    "    return clusters, log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster assignment\n",
    "clusters1, logL1 = assign_to_cluster_gmm(X, distr0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando dados atribuídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new distributions\n",
    "distr1 = compute_distributions(X, clusters1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plot_assignment_gmm(X, clusters1, distr1, 1, logL1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectativa - Maximização\n",
    "Os dois passos que acabamos de ver fazem parte de uma família mais ampla de algoritmos para maximizar probabilidades, chamados algoritmos de maximização de expectativas .\n",
    "\n",
    "Na etapa de expectativa, calculamos a expectativa dos parâmetros, dada a atribuição atual do cluster.\n",
    "\n",
    "Na etapa de maximização, atribuímos observações ao cluster que maximizaram a probabilidade da observação única.\n",
    "\n",
    "O procedimento alternativo, mais intensivo em termos computacionais, teria sido especificar uma função de verossimilhança global e encontrar os parâmetros de média e variância das duas distribuições normais que maximizaram essas verossimilhanças."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo completo\n",
    "Agora podemos implementar o algoritmo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_manual(X, K):\n",
    "\n",
    "    # Init\n",
    "    i = 0\n",
    "    logL0 = 1e4\n",
    "    logL1 = 1e5\n",
    "    clusters = np.random.randint(K,size=(np.size(X,0)))\n",
    "\n",
    "    # Iterate until convergence\n",
    "    while np.abs(logL0-logL1) > 1e-10:\n",
    "        logL1 = logL0\n",
    "        distr = compute_distributions(X, clusters)\n",
    "        clusters, logL0 = assign_to_cluster_gmm(X, distr)\n",
    "        plot_assignment_gmm(X, clusters, distr, i, logL0)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando o agrupamento k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "gmm_manual(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso, o GMM faz um trabalho muito ruim na identificação dos clusters originais.\n",
    "\n",
    "Clusters sobrepostos\n",
    "Vamos agora tentar com um conjunto de dados diferente, onde os dados são extraídos de duas distribuições gaussianas bivariadas sobrepostas, formando uma cruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "X = np.random.randn(50,2)\n",
    "X[0:25, :] = np.random.multivariate_normal([0,0], [[50,0],[0,1]], size=25)\n",
    "X[25:, :] = np.random.multivariate_normal([0,0], [[1,0],[0,50]], size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_figure_1(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM com distribuições sobrepostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GMM\n",
    "gmm_manual(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, o GMM é capaz de recuperar corretamente os clusters originais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means com distribuições sobrepostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-means\n",
    "kmeans_manual(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means gera clusters completamente diferentes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
